{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbfb8998",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fd2143e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "#    #Paths\n",
    "DATA_PATH = \"C:/Users/User/Documents/paps/\"\n",
    "COORD_STATIC_DATA_PATH = DATA_PATH + \"pomiary/prepared/coordinates_data_static_merged.csv\"\n",
    "REF_STATIC_DATA_PATH = DATA_PATH + \"pomiary/prepared/reference_data_static_merged.csv\"\n",
    "COORD_DYNAMIC_DATA_PATH = DATA_PATH + \"pomiary/prepared/coordinates_data_dynamic_merged.csv\"\n",
    "REF_DYNAMIC_DATA_PATH = DATA_PATH + \"pomiary/prepared/reference_data_dynamic_merged.csv\"\n",
    "MODEL_DIR = \"models/best_model.hdf5\"\n",
    "MODEL_WEIGHTS_PATH = \"models/trained_weights.hdf5\"\n",
    "\n",
    "#    #Model parameters\n",
    "BATCH_SIZE = 512\n",
    "N_EPOCHS = 200\n",
    "\n",
    "#    #Automatic removing models in case of re-learning\n",
    "if os.path.exists(MODEL_DIR):\n",
    "    os.remove(MODEL_DIR)\n",
    "\n",
    "if os.path.exists(MODEL_WEIGHTS_PATH):\n",
    "    os.remove(MODEL_WEIGHTS_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b415932a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensorflow version:  2.3.0\n",
      "1  GPU is avaiable \n",
      "Tensorflow with NVIDIA CUDA support.\n"
     ]
    }
   ],
   "source": [
    "#    #Checking GPU availability for more computing power\n",
    "print(\"Tensorflow version: \", tf.__version__)\n",
    "\n",
    "if tf.test.is_built_with_cuda():\n",
    "    physical_device = tf.config.list_physical_devices('GPU')\n",
    "    tf.config.experimental.set_memory_growth(physical_device[0], True)\n",
    "    print(len(physical_device), \" GPU is avaiable \",)\n",
    "    print(\"Tensorflow with NVIDIA CUDA support.\")\n",
    "else:\n",
    "    print(\"Tensorflow without NVIDIA CUDA support.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "14e90a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Functions\n",
    "def plot_results(model_history, metric='mean_squared_error'):\n",
    "    metric = metric\n",
    "    val_metric = 'val_' + metric\n",
    "    \n",
    "    accuracy = model_history.history[metric]\n",
    "    val_accuracy = model_history.history[val_metric]\n",
    "    loss = model_history.history['loss']\n",
    "    val_loss = model_history.history['val_loss']\n",
    "    epochs = range(1, len(accuracy) + 1)\n",
    "\n",
    "    fig, axs = plt.subplots(1, 2, figsize = (12, 4))\n",
    "    axs[0].plot(epochs, accuracy, 'b', label = 'Training MSE')\n",
    "    axs[0].plot(epochs, val_accuracy, 'r', label = 'Validation MSE')\n",
    "    axs[0].set_title('Model MSE')\n",
    "    axs[0].set(xlabel = 'Epochs', ylabel = 'MSE')\n",
    "    axs[0].legend()\n",
    "    \n",
    "    axs[1].plot(epochs, loss, 'b', label = 'Training loss')\n",
    "    axs[1].plot(epochs, val_loss, 'r', label = 'Validation loss')\n",
    "    axs[1].set_title('Model loss')\n",
    "    axs[1].set(xlabel = 'Epochs', ylabel = 'Loss')\n",
    "    axs[1].legend()\n",
    "    \n",
    "def create_nnetwork_block_1(input_tensor):\n",
    "    x = layers.Lambda(lambda x: x, output_shape=(1,))(input_tensor)\n",
    "    x = layers.Dense(1, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(64, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(256, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(512, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(64, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def create_nnetwork_block_2(input_tensor):\n",
    "    x = layers.Lambda(lambda x: x, output_shape=(1,))(input_tensor)\n",
    "    x = layers.Dense(1, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(256, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(512, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(512, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(512, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(256, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(256, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(128, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(64, activation=\"elu\", kernel_initializer=\"he_normal\")(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "    \n",
    "    return x\n",
    "\n",
    "def count_errors(predictions, references):\n",
    "    errors = []\n",
    "    for i in range(len(predictions)):\n",
    "        tmp_x = references[i][0] - predictions[i][0]\n",
    "        tmp_y = references[i][1] - predictions[i][1]\n",
    "        errors.append(np.sqrt(tmp_x ** 2 + tmp_y ** 2))\n",
    "    return errors\n",
    "\n",
    "def ecdf(data):\n",
    "    x = np.sort(data)\n",
    "    n = x.size\n",
    "    y = np.arange(1, n+1) / n\n",
    "    return [x, y]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1182d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Reading data from files\n",
    "coordinates_static_data = pd.read_csv(COORD_STATIC_DATA_PATH)\n",
    "reference_static_data = pd.read_csv(REF_STATIC_DATA_PATH)\n",
    "#    #Creating arrays with read data\n",
    "coordinates_static_data = np.array(coordinates_static_data, dtype=\"float32\")\n",
    "reference_static_data = np.array(reference_static_data, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a37b54c7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_test_split' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_18844/2019758397.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m#    #Splitting arrays and shuffling their data for better learning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mcoordinates_learn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates_value\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_learn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_value\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcoordinates_static_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreference_static_data\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m128\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mcoordinates_learn_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates_learn_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoordinates_learn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates_learn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mcoordinates_value_1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates_value_2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcoordinates_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcoordinates_value\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_test_split' is not defined"
     ]
    }
   ],
   "source": [
    "#    #Splitting arrays and shuffling their data for better learning\n",
    "coordinates_learn, coordinates_value, reference_learn, reference_value = train_test_split(coordinates_static_data, reference_static_data, shuffle=True, random_state=128)\n",
    "\n",
    "coordinates_learn_1, coordinates_learn_2 = coordinates_learn[:,0], coordinates_learn[:,1]\n",
    "coordinates_value_1, coordinates_value_2 = coordinates_value[:,0], coordinates_value[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b851c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Displaying shape of arrays\n",
    "coordinates_learn.shape, coordinates_value.shape, reference_learn.shape, reference_value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8ea6951",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#    #Instantiation a Keras tensor that allow to build a Keras model by knowing the input shape\n",
    "input_layer_1 = layers.Input((1,))\n",
    "input_layer_2 = layers.Input((1,))\n",
    "#    #Creating two blocks of neural network with specified hidden layers and neurons in them\n",
    "nnetwork_block_1 = create_nnetwork_block_1(input_layer_1)\n",
    "nnetwork_block_2 = create_nnetwork_block_2(input_layer_2)\n",
    "#    #Merging two blocks to create one output for whole neural network\n",
    "nnetwork_merged = layers.Concatenate(axis=1)([nnetwork_block_1, nnetwork_block_2])\n",
    "#    #Groups the layers into an object with learning and conclusion features.\n",
    "model = tf.keras.models.Model(inputs=[input_layer_1, input_layer_2], outputs=[nnetwork_merged])\n",
    "#    #Preview a summary of the network.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15d8d389",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Creating parameters for callbacks:\n",
    "#    #EarlyStopping - stop learning when a monitored metric has stopped improving\n",
    "#    #    #monitor - monitoring value losses\n",
    "#    #    #patience - number of epochs with no improvement after which training will be stopped\n",
    "#    #    #mode - in 'min' mode training will stop when the quantity monitored has stopped decreasing\n",
    "#    #ReduceLROnPlateau - reduce learning rate when a metric has stopped improving\n",
    "#    #    #monitor - monitoring value losses\n",
    "#    #    #factor - factor by which the learning rate will be reduced\n",
    "#    #    #patience - number of epochs with no improvement after which learning rate will be reduced\n",
    "#    #    #min_lr - lower bound on the learning rate.\n",
    "#    #ModelCheckpoint - is used to save a model or weights\n",
    "#    #    #filepath - path to save the model file\n",
    "#    #    #monitor - monitoring value losses\n",
    "#    #    #mode - the decision to overwrite the current save file based on the minimization of the monitored quantity.\n",
    "#    #    #save_best_only - it only saves when the model is considered the \"best\" and the latest best model according to the quantity monitored will not be overwritten\n",
    "\n",
    "callback_params = [\n",
    "    callbacks.EarlyStopping(monitor='val_loss', patience=10, mode=\"min\"),\n",
    "    callbacks.ReduceLROnPlateau(monitor = 'val_loss', factor=0.1, patience=5, min_lr=0.001),\n",
    "    callbacks.ModelCheckpoint(filepath=MODEL_DIR, monitor='val_loss', mode='min', save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acbf1d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Configurating the model for training\n",
    "#    #    #optimizer - optimizer instance\n",
    "#    #    #loss - Loss function\n",
    "#    #    #metrics - List of metrics to be evaluated by the model during training and testing - MSE\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(), \n",
    "              loss=tf.keras.losses.Huber(),\n",
    "              metrics=[tf.keras.metrics.MeanSquaredError()])\n",
    "#    #Training the model for a fixed number of epochs based on input and saving it\n",
    "#    #    #x - input data\n",
    "#    #    #y - target data\n",
    "#    #    #batch_size - number of samples per numeric calculation allowing to know how to adjust the parameters of a network in a way that its output error is minimized. (gradient)\n",
    "#    #    #epochs - max number of epochs to train the model\n",
    "#    #    #callbacks - list of callbacks instances to apply during learning\n",
    "#    #    #validation_data - data on which to evaluate the loss and any model metrics at the end of each epoch, model will not be learned on this data\n",
    "#    #    #shuffle - whether to shuffle the training data before each epoch\n",
    "learning_process = model.fit(x=(coordinates_learn_1, coordinates_learn_2),\n",
    "                    y=reference_learn,\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=N_EPOCHS,\n",
    "                    callbacks = callback_params,\n",
    "                    validation_data = ((coordinates_value_1, coordinates_value_2), reference_value),\n",
    "                    shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16543364",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Graphs about whole learning process\n",
    "plot_results(learning_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d97afaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Importing merged dynamic data from .csv file\n",
    "coordinates_dynamic_data = pd.read_csv(COORD_DYNAMIC_DATA_PATH)\n",
    "reference_dynamic_data = pd.read_csv(REF_DYNAMIC_DATA_PATH)\n",
    "#    #Merged dynamic data allocation to an array \n",
    "coordinates_dynamic = np.array(coordinates_dynamic_data, dtype=\"float32\")\n",
    "reference_dynamic = np.array(reference_dynamic_data, dtype=\"float32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a01be357",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Calculating The loss value & MSE values before and after model learning\n",
    "mse = tf.keras.metrics.MeanSquaredError()\n",
    "print(f\"\\nNew MSE:{model.evaluate((coordinates_dynamic[:,0], coordinates_dynamic[:,1]), reference_dynamic)[1]}\\nOld MSE:{mse(reference_dynamic, coordinates_dynamic).numpy()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e163dfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Saving the best modified weights based on neural network \n",
    "model.save_weights(\"models/trained_weights.hdf5\", save_format=\"hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbe38fb",
   "metadata": {},
   "source": [
    "## Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a42d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Generating output predictions for the dynamic samples\n",
    "model_predictions = model.predict((coordinates_dynamic[:,0], coordinates_dynamic[:,1]))\n",
    "#    #Counting errors for dynamic samples\n",
    "errors_dynamic = count_errors(coordinates_dynamic, reference_dynamic)\n",
    "#    #Counting errors for model from output predictions based on dynamic samples\n",
    "errors_model_predicted = count_errors(model_predictions, reference_dynamic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c10c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Calculating error distribution of dynamic data\n",
    "dist_dynamic = ecdf(errors_dynamic)\n",
    "#    #Calculating error distribution of result obtained by using a neural network.\n",
    "dist_model_predicted = ecdf(errors_model_predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc64ff4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,7))\n",
    "plt.plot(dist_dynamic[0], dist_dynamic[1], linewidth=4, label=\"Error distribution of dynamic data\")\n",
    "plt.plot(dist_model_predicted[0], dist_model_predicted[1], linewidth=4, label=\"Error distribution of Neural network\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afad48dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Preview of weights data\n",
    "model.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74bafb14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#    #Saving weights data to .txt file\n",
    "file = open(\"final_weights.txt\", \"w\")\n",
    "for line in model.weights:\n",
    "    file.writelines(str(line))\n",
    "file.close()\n",
    "print(\"Weights saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "781f2585",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
